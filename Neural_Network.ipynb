{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bf3f35",
   "metadata": {},
   "source": [
    "## Neutral Network Model for CA Housing Dataset\n",
    "\n",
    "#### 1. Prepare Data\n",
    "Model specific pre-processing, including creating datasets and scaling data for neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5175ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14703\n",
      "Test size: 3676\n",
      "       longitude  latitude  median_income  median_house_value ocean_proximity  \\\n",
      "1310     -122.10     37.89         5.2079            310300.0        NEAR BAY   \n",
      "16156    -122.10     37.40         4.3077            293500.0        NEAR BAY   \n",
      "10301    -118.07     33.80         7.1221            384500.0       <1H OCEAN   \n",
      "8823     -121.07     39.09         2.8864            143100.0          INLAND   \n",
      "10428    -120.07     39.24         4.9620            169500.0          INLAND   \n",
      "\n",
      "       ocean_proximity_encoded  longitude_scaled  latitude_scaled  \\\n",
      "1310                         2         -1.318283         1.069327   \n",
      "16156                        2         -1.318283         0.840931   \n",
      "10301                        3          0.716643        -0.837078   \n",
      "8823                         4         -0.798191         1.628663   \n",
      "10428                        4         -0.293246         1.698580   \n",
      "\n",
      "       median_income_scaled  median_house_value_scaled  \n",
      "1310               0.963696                   1.258981  \n",
      "16156              0.391710                   1.083829  \n",
      "10301              2.179977                   2.032568  \n",
      "8823              -0.511382                  -0.484197  \n",
      "10428              0.807452                  -0.208958  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv('datasets/preprocessed_dataset.csv')\n",
    "\n",
    "# Step 1: First split (Train: 80%, Test: 20%)\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check sizes\n",
    "print(f\"Train size: {len(train_set)}\")\n",
    "print(f\"Test size: {len(test_set)}\")\n",
    "\n",
    "# Select the columns to scale (last two columns)\n",
    "cols_to_scale = ['longitude', 'latitude', 'median_income', 'median_house_value']\n",
    "\n",
    "# Initialize scaler and fit on training data only\n",
    "scaler = StandardScaler()\n",
    "train_scaled_values = scaler.fit_transform(train_set[cols_to_scale])\n",
    "test_scaled_values = scaler.transform(test_set[cols_to_scale])\n",
    "\n",
    "# Create new column names\n",
    "scaled_col_names = [f\"{col}_scaled\" for col in cols_to_scale]\n",
    "\n",
    "# Add scaled columns and drop originals\n",
    "for df, scaled_data in zip([train_set, test_set], [train_scaled_values, test_scaled_values]):\n",
    "    for orig_col, scaled_col in zip(cols_to_scale, scaled_col_names):\n",
    "        df[scaled_col] = scaled_data[:, scaled_col_names.index(scaled_col)]\n",
    "\n",
    "print(train_set.head())\n",
    "\n",
    "# Select features for model\n",
    "feature_cols = ['longitude_scaled', 'latitude_scaled', 'ocean_proximity_encoded', 'median_income_scaled',]\n",
    "target_col = 'median_house_value_scaled'\n",
    "\n",
    "#feature_cols = ['longitude_scaled', 'latitude_scaled']\n",
    "#target_col = 'ocean_proximity_encoded'\n",
    "\n",
    "# Extract features and targets and convert to numpy\n",
    "X_train = train_set[feature_cols].to_numpy()\n",
    "y_train = train_set[target_col].to_numpy()\n",
    "X_test = test_set[feature_cols].to_numpy()\n",
    "y_test = test_set[target_col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa17e7b",
   "metadata": {},
   "source": [
    "#### Define Model Parameters\n",
    "1. Define **Hyperparameter** Set for **Grid Search**\n",
    "2. Define model --> use grid search to **optimize/train model**\n",
    "3. **Test model** and report results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "70f77278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters found: {'activation': 'tanh', 'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': (16, 64, 128, 64, 16), 'max_iter': 1000, 'n_iter_no_change': 50, 'solver': 'adam', 'validation_fraction': 0.2}\n",
      "Test MSE with best model: 0.2387\n",
      "Test MAPE with best model : 1.6932\n",
      "Test R² score: 0.7573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Define hyperparameters to search over\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ # Many iterations were run to find a good set of layer sizes to test\n",
    "        (16, 32, 32, 16),   # (Stoped once improvments became marginal)\n",
    "        (16,32,64,32,16), \n",
    "        (16,64,128,64,16), \n",
    "        (32,64,128,64,32)\n",
    "    ], \n",
    "    'activation': ['relu', 'tanh'], # Tested different Activation Functions\n",
    "    'alpha': [0.0001, 0.001],       # Regularizarion values to test\n",
    "    'solver': ['adam'],             # Always use adam optimization model\n",
    "    'max_iter': [1000],             # Alwayys use 1000 iterations\n",
    "    'early_stopping': [True],       # Use validation set to benchmark model performance and limit overfitting\n",
    "    'n_iter_no_change' : [50],      # High Validation Patience to overcome it stopping too early\n",
    "    'validation_fraction': [0.2]    # 20% of training data used as validation during training\n",
    "}\n",
    "\n",
    "# Define base model\n",
    "mlp = MLPRegressor(random_state=42,)\n",
    "\n",
    "# Perform Grid Search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit only on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate best model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "test_mape = mean_absolute_percentage_error(y_test, test_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "print(f\"Test MAPE with best model : {test_mape:.4f}\")\n",
    "print(f\"Test R² score: {test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
